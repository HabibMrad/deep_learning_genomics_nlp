{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing AlexNet with convolution approach\n",
    "\n",
    "This is the same notebook as `Generalization_CIFAR10_AlexNet` except the following regularization scheme is implemented:\n",
    "\n",
    "1. Each real data point is replaced by $K$ Gaussian samples around that data point.\n",
    "2. Of the $K$ samples, some proportion $p_a$ of them are labeled incorrectly (hence \"adversarial\" regularizaiton)\n",
    "3. New samples are generated each epoch\n",
    "\n",
    "This means the following functions (which includes all the ones that depend on $K$ during graph building) are changed:\n",
    "\n",
    "- `loss`\n",
    "- `acc`\n",
    "- `train`\n",
    "- `append_noisy_samples`\n",
    "- `graph_builder_wrapper`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time,os,pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from chiyuan_cifar10_jz import chiyuan,cifar10_parameters \n",
    "from sklearn.utils import shuffle\n",
    "np.set_printoptions(precision=2,suppress=True)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The small Alexnet is constructed by two (convolution 5x5\n",
    "# → max-pool 3x3 → local-response-normalization) modules followed by two fully connected layers\n",
    "# with 384 and 192 hidden units, respectively. Finally a 10-way linear layer is used for prediction\n",
    "\n",
    "def alexnet_arch(NUM_CLASSES):\n",
    "    \n",
    "    input_data = tf.placeholder(tf.float32, shape=[None, 28, 28, 3])\n",
    "    input_labels = tf.placeholder(tf.float32, shape=[None,10])\n",
    "    \n",
    "    # conv1\n",
    "    with tf.variable_scope('conv1') as scope:\n",
    "        kernel = tf.get_variable('weights',[5, 5, 3, 64],\n",
    "                                 initializer=tf.truncated_normal_initializer(stddev=1e-2))\n",
    "        conv = tf.nn.conv2d(input_data, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        biases = tf.get_variable('biases', [64], \n",
    "                                 initializer=tf.truncated_normal_initializer(stddev=1e-2))\n",
    "        pre_activation = tf.nn.bias_add(conv, biases)\n",
    "        conv1 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "\n",
    "    # pool1\n",
    "    pool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],\n",
    "                           padding='VALID', name='pool1')\n",
    "    # norm1\n",
    "    norm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,\n",
    "                      name='norm1')\n",
    "\n",
    "    # conv2\n",
    "    with tf.variable_scope('conv2') as scope:\n",
    "        kernel = tf.get_variable('weights',[5, 5, 64, 64],\n",
    "                                 initializer=tf.truncated_normal_initializer(stddev=1e-2))\n",
    "        conv = tf.nn.conv2d(norm1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        biases = tf.get_variable('biases', [64], \n",
    "                                 initializer=tf.truncated_normal_initializer(stddev=1e-2))\n",
    "        pre_activation = tf.nn.bias_add(conv, biases)\n",
    "        conv2 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "\n",
    "    # pool2\n",
    "    pool2 = tf.nn.max_pool(conv2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],\n",
    "                           padding='VALID', name='pool2')\n",
    "\n",
    "    # norm2\n",
    "    norm2 = tf.nn.lrn(pool2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,\n",
    "                      name='norm2')\n",
    "\n",
    "\n",
    "    # local3\n",
    "    with tf.variable_scope('local3') as scope:\n",
    "        # Move everything into depth so we can perform a single matrix multiply.\n",
    "        reshape = tf.reshape(norm2, [-1, 6*6*64])\n",
    "        dim = reshape.get_shape()[1].value\n",
    "        weights = tf.get_variable('weights',[dim, 384],\n",
    "                                  initializer=tf.truncated_normal_initializer(stddev=4e-2))\n",
    "        biases = tf.get_variable('biases', [384], \n",
    "                                 initializer=tf.truncated_normal_initializer(stddev=4e-2))\n",
    "        local3 = tf.nn.relu(tf.matmul(reshape, weights) + biases, name=scope.name)\n",
    "\n",
    "    # local4\n",
    "    with tf.variable_scope('local4') as scope:\n",
    "        weights = tf.get_variable('weights',[384, 192],\n",
    "                                  initializer=tf.truncated_normal_initializer(stddev=4e-2))\n",
    "        biases = tf.get_variable('biases', [192], \n",
    "                                 initializer=tf.truncated_normal_initializer(stddev=4e-2))\n",
    "        local4 = tf.nn.relu(tf.matmul(local3, weights) + biases, name=scope.name)\n",
    "\n",
    "    # linear layer(WX + b)\n",
    "    with tf.variable_scope('linear') as scope:\n",
    "        weights = tf.get_variable('weights',[192, NUM_CLASSES],\n",
    "                                  initializer=tf.truncated_normal_initializer(stddev=1/192.0))\n",
    "        biases = tf.get_variable('biases', [NUM_CLASSES], \n",
    "                                 initializer=tf.truncated_normal_initializer(stddev=1/192.0))\n",
    "        linear = tf.add(tf.matmul(local4, weights), biases, name=scope.name)\n",
    "        \n",
    "    return input_data,input_labels,linear\n",
    "\n",
    "def loss(g, Y):\n",
    "    return tf.reduce_mean(tf.reduce_sum(tf.pow(g-Y,2),1))\n",
    "\n",
    "# Accuracy\n",
    "def acc(g, Y):\n",
    "    correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(g, 1))\n",
    "    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use some image preprocessing stuff from chiyuan\n",
    "#   - images are cropped to be 28x28x3 from 32x32x3\n",
    "#   - images are also whitened \n",
    "\n",
    "def cifar_one_hot(i):\n",
    "        v = np.zeros(10)\n",
    "        v[i] = 1\n",
    "        return v\n",
    "    \n",
    "def get_cifar10_dataset(p_corrupt_label,n_samps=50000,rand_seed=None):\n",
    "    \n",
    "    #  'SubS:tr:50000' means draw a random subset of 50000 samples for training (no replace)\n",
    "    #  'RndL:trtt:100' means corrupt the train and test set labels with 100% probability\n",
    "    class params(cifar10_parameters):\n",
    "        def __init__(self,p,rand_seed,n_samp=50000):\n",
    "            self.dataset = 'cifar10|SubS:tr:%s|RndL:trtt:%s'%(int(n_samp),int(p))\n",
    "            self.rand_seed = rand_seed\n",
    "#             self.per_image_whitening = False\n",
    "    \n",
    "    p = params(p_corrupt_label,rand_seed,n_samp=n_samps)\n",
    "    c = chiyuan(p)\n",
    "    _, datasets = c.prepare_inputs()\n",
    "    return datasets[0][0],np.array(map(cifar_one_hot,datasets[0][1])), \\\n",
    "           datasets[1][0],np.array(map(cifar_one_hot,datasets[1][1]))\n",
    "\n",
    "def graph_builder_wrapper(num_classes,sd,save_dir,lr_initial=0.01):\n",
    "    \n",
    "    input_data, input_labels, g_out = alexnet_arch(num_classes)\n",
    "    saver = tf.train.Saver(max_to_keep=20)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    total_loss = loss(g_out, input_labels)\n",
    "    learning_rate = tf.Variable(lr_initial, name='learning_rate')\n",
    "    opt_step = tf.train.MomentumOptimizer(learning_rate,0.9).minimize(total_loss)\n",
    "    tf.summary.scalar('loss', total_loss)\n",
    "    \n",
    "    # Accuracy\n",
    "    total_acc = acc(g_out, input_labels)\n",
    "    tf.summary.scalar('accuracy', total_acc)\n",
    "    \n",
    "    # Add histograms for trainable variables.\n",
    "    for var in tf.trainable_variables():\n",
    "        tf.summary.histogram(var.op.name, var)\n",
    "\n",
    "    # Merge all the summaries and write them out to save_dir\n",
    "    merged = tf.summary.merge_all()\n",
    "    train_writer = tf.summary.FileWriter(save_dir + '/train')\n",
    "    valid_writer = tf.summary.FileWriter(save_dir + '/validation')\n",
    "    \n",
    "    graph = dict( \n",
    "        input_data = input_data,\n",
    "        input_labels = input_labels,\n",
    "        total_loss = total_loss,\n",
    "        total_acc = total_acc,\n",
    "        g_out = g_out,\n",
    "        opt_step = opt_step,\n",
    "        learning_rate = learning_rate,\n",
    "        merged = merged,\n",
    "        train_writer = train_writer,\n",
    "        valid_writer = valid_writer,\n",
    "        saver = saver\n",
    "    )\n",
    "    \n",
    "    return graph\n",
    "\n",
    "# Augment dataset with perturbed samples and labels\n",
    "def append_noisy_samples(X,Y,n_classes,K=10,pa=0.0,stddev=0.1):\n",
    "    perturbations = np.random.normal(0,stddev,[len(X)*K]+list(np.shape(X))[1:])\n",
    "    \n",
    "    def adversarial_label_expand(y):\n",
    "        y = np.argmax(y)\n",
    "        y_ = np.repeat(y,K)\n",
    "        inds = np.random.choice(range(K),int(pa*K),replace=False)\n",
    "        p = np.ones(num_classes)\n",
    "        p[y] = 0\n",
    "        p /= num_classes-1.\n",
    "        y_[inds] = np.random.choice(range(num_classes),int(pa*K),p=p)\n",
    "        return np.array(map(cifar_one_hot,y_))\n",
    "    \n",
    "    X_ = np.repeat(X,K,axis=0)+perturbations\n",
    "    Y_ = np.vstack(map(adversarial_label_expand,Y))\n",
    "    return X_,Y_\n",
    "\n",
    "def train(Xtr,Ytr,Xva,Yva,graph,sd,K,pa,save_dir,num_epochs=100,batch_size=100,save_every=1,verbose=True):\n",
    "    \n",
    "    training_losses,training_accs = [],[]\n",
    "    validation_losses,validation_accs = [],[]\n",
    "    num_classes = len(np.unique(Ytr))\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            lr = 0.01*0.95**(epoch/390.) # initial lr * decay rate ^(step/decay_steps)\n",
    "            sess.run(graph['learning_rate'].assign(lr))\n",
    "\n",
    "            t = time.time()\n",
    "            training_loss = 0\n",
    "            training_acc = 0\n",
    "            steps = 0.\n",
    "            Xtr_, Ytr_ = shuffle(Xtr,Ytr)\n",
    "\n",
    "            if len(Xtr_)%batch_size == 0: end = len(Xtr_)\n",
    "            else: end = len(Xtr_)-batch_size\n",
    "            for i in range(0,end,batch_size):\n",
    "\n",
    "                # Generate noisy samples for convolution estimation..\n",
    "                x,y = append_noisy_samples(Xtr_[i:i+batch_size], \n",
    "                                           Ytr_[i:i+batch_size], \n",
    "                                           num_classes, K=K, pa=pa, stddev=sd)\n",
    "\n",
    "                feed_dict = {graph['input_data']: x, graph['input_labels']: y}\n",
    "\n",
    "                summary,training_loss_,training_acc_,_ = sess.run([graph['merged'],graph['total_loss'],\n",
    "                                                                   graph['total_acc'],graph['opt_step']],\n",
    "                                                                  feed_dict=feed_dict)\n",
    "                training_loss += training_loss_\n",
    "                training_acc += training_acc_\n",
    "                steps += 1.\n",
    "\n",
    "                if verbose:\n",
    "                    print('\\rTraining batch %s/%s (%.3f s): loss %.3f, acc %.3f' \\\n",
    "                          %(steps,len(Xtr_)/batch_size,time.time()-t,training_loss_,training_acc_),end='')            \n",
    "            \n",
    "            if epoch%save_every == 0: \n",
    "                graph['train_writer'].add_summary(summary, epoch)\n",
    "                \n",
    "                # Get results on test set\n",
    "                feed_dict = {graph['input_data']: Xva, graph['input_labels']: Yva}\n",
    "                summary,validation_loss,validation_acc = sess.run([graph['merged'],graph['total_loss'],\n",
    "                                                                   graph['total_acc']],\n",
    "                                                                  feed_dict=feed_dict)\n",
    "                graph['valid_writer'].add_summary(summary, epoch)\n",
    "                validation_losses.append(validation_loss)\n",
    "                validation_accs.append(validation_acc)\n",
    "                \n",
    "                if not os.path.exists(save_dir+'checkpoints/'): os.mkdir(save_dir+'checkpoints/')\n",
    "                graph['saver'].save(sess,save_dir+'checkpoints/epoch'+str(epoch))\n",
    "\n",
    "            training_losses.append(training_loss/steps)\n",
    "            training_accs.append(training_acc/steps)\n",
    "            \n",
    "            if verbose:\n",
    "                print('\\rEpoch: %s/%s done (Learning Rate: %.6f  Training Loss: %.3f)' \\\n",
    "                      %(epoch+1,num_epochs,lr,training_losses[-1]))\n",
    "                \n",
    "    return training_losses,training_accs,validation_losses,validation_accs\n",
    "\n",
    "\n",
    "# Use trained model to predict\n",
    "def predict(X,Y,graph,save_dir,batch_size=100,verbose=False,return_all=False):\n",
    "\n",
    "    # Load from checkpoint corresponding to latest epoch \n",
    "    max_epoch = max([int(f.split('epoch')[1].split('.')[0]) for f in os.listdir(save_dir+'checkpoints/') if 'epoch' in f])\n",
    "    t = time.time()\n",
    "    with tf.Session() as sess:\n",
    "        graph['saver'].restore(sess,save_dir+'checkpoints/epoch%s'%(max_epoch))\n",
    "        \n",
    "        embedding = np.zeros((len(X),num_classes))\n",
    "        overall_loss = 0\n",
    "        overall_acc = 0\n",
    "        steps = 0.\n",
    "        for i in range(0,len(X),batch_size):\n",
    "            g_,loss_,acc_ = sess.run([graph['g_out'],graph['total_loss'],graph['total_acc']], \n",
    "                                      feed_dict = {graph['input_data']:X[i:i+batch_size],\n",
    "                                                   graph['input_labels']:Y[i:i+batch_size]})\n",
    "            embedding[i:i+batch_size,:] = g_\n",
    "            overall_loss += loss_\n",
    "            overall_acc += acc_\n",
    "            steps += 1\n",
    "            if verbose: print('\\r%s/%s samples processed.. (%.3f s elapsed)'%(i+1,len(X),time.time()-t),end='')\n",
    "\n",
    "        overall_loss /= steps\n",
    "        overall_acc /= steps\n",
    "        \n",
    "    if verbose: print('\\nOverall accuracy (argmax of embeddings as labels): %.3f'%(overall_acc))\n",
    "    if return_all: return overall_acc,overall_loss,embedding\n",
    "    return overall_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100 done (Learning Rate: 0.010000  Training Loss: 0.913)\n",
      "Epoch: 2/100 done (Learning Rate: 0.009999  Training Loss: 0.889)\n",
      "Epoch: 3/100 done (Learning Rate: 0.009997  Training Loss: 0.846)\n",
      "Training batch 41.0/50 (16.152 s): loss 0.808, acc 0.373"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "num_epochs = 100\n",
    "batch_size = 100\n",
    "K = 20\n",
    "n_samps = 5000\n",
    "\n",
    "list_p_corrupt_label = [0,100]\n",
    "list_sd = [0.1,0.5,1,2]\n",
    "list_pa = [0.0,0.2,0.5]\n",
    "\n",
    "train_accs = np.zeros((len(list_p_corrupt_label),len(list_sd),len(list_pa)))\n",
    "test_accs = np.zeros((len(list_p_corrupt_label),len(list_sd),len(list_pa)))\n",
    "\n",
    "start = time.time()\n",
    "for i,p_corrupt_label in enumerate(list_p_corrupt_label):\n",
    "    \n",
    "    Xtr,Ytr,Xtt,Ytt = get_cifar10_dataset(p_corrupt_label,n_samps=n_samps)\n",
    "    \n",
    "    for j,sd in enumerate(list_sd):\n",
    "        for k,pa in enumerate(list_pa):\n",
    "\n",
    "            save_dir = './temp/alexnet_K%s_sd%s_p%s_pa%s/'%(K,sd,p_corrupt_label,pa)\n",
    "\n",
    "            tf.reset_default_graph()\n",
    "            \n",
    "            graph = graph_builder_wrapper(num_classes,sd,save_dir,lr_initial=0.01)\n",
    "\n",
    "            if 'checkpoints' not in os.listdir(save_dir):\n",
    "                tr_losses,tr_accs,va_losses,va_accs = train(Xtr,Ytr,Xtt,Ytt,graph,sd,K,pa,save_dir,\n",
    "                                                            num_epochs=num_epochs,batch_size=batch_size)\n",
    "                train_accs[i,j,k] = tr_accs[-1]\n",
    "                test_accs[i,j,k] = va_accs[-1]\n",
    "            else:\n",
    "                train_accs[i,j,k] = predict(Xtr,Ytr,graph,save_dir,batch_size=batch_size)\n",
    "                test_accs[i,j,k] = predict(Xtt,Ytt,graph,save_dir,batch_size=batch_size)\n",
    "\n",
    "            print('-'*80)\n",
    "            print('Finished with K = %s, p_corrupt_label = %.3f, sd = %.2f, pa = %.2f (%.3f s elapsed)' \\\n",
    "                  %(K,p_corrupt_label,sd,pa,time.time()-start))\n",
    "            print('  Train acc: %.2f, Test acc: %.2f'%(train_accs[i,j,k],test_accs[i,j,k]))\n",
    "            print('-'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(16,4))\n",
    "# plt.subplot(1,2,1)\n",
    "# plt.plot(tr_losses)\n",
    "# plt.plot(va_losses)\n",
    "# plt.grid()\n",
    "# plt.title('Losses')\n",
    "# plt.subplot(1,2,2)\n",
    "# plt.plot(tr_accs,label='Train')\n",
    "# plt.plot(va_accs,label='Valid')\n",
    "# plt.grid()\n",
    "# plt.title('Accuracies')\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # For Debugging\n",
    "# tf.reset_default_graph()\n",
    "\n",
    "# x = tf.constant([[10.,10.],[10.,10.]])\n",
    "# y = tf.constant([[10.2,10.1],[10.2,10.1]])\n",
    "# l = tf.reduce_sum(tf.pow(x-y,2),1)\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#     print(sess.run(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Testing to make sure the loss function is written correctly..\n",
    "# K = 40\n",
    "# n = 10\n",
    "# d = 2\n",
    "# Z = np.random.choice(range(10),(n,d))\n",
    "# Zp = append_noisy_samples(Z,K=K,stddev=0.01)\n",
    "# Z_ = np.mean(Zp.T.reshape(-1,K),1).reshape(d,-1).T\n",
    "# print(Z)\n",
    "# print(Z_)\n",
    "\n",
    "# z = tf.placeholder(tf.float32, shape=[n*K,d])\n",
    "# z_ = tf.transpose(tf.reshape(tf.reduce_mean(tf.reshape(tf.transpose(z),[-1,K]),1),[d,-1]))\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#     test = sess.run(z_, feed_dict = {z: Zp})\n",
    "    \n",
    "# print(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
